{"componentChunkName":"component---node-modules-gatsby-theme-academic-src-templates-post-post-jsx","path":"/research/llm_security/","result":{"data":{"mdx":{"timeToRead":1,"tableOfContents":{},"frontmatter":{"cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/fc9b9cb73e88de91feb2a35530f95fb0/73b88/llm-attack.png","srcSet":"/static/fc9b9cb73e88de91feb2a35530f95fb0/bb9ab/llm-attack.png 750w,\n/static/fc9b9cb73e88de91feb2a35530f95fb0/73b88/llm-attack.png 1000w,\n/static/fc9b9cb73e88de91feb2a35530f95fb0/043a4/llm-attack.png 1024w","sizes":"(min-width: 1000px) 1000px, 100vw"},"sources":[{"srcSet":"/static/fc9b9cb73e88de91feb2a35530f95fb0/43648/llm-attack.webp 750w,\n/static/fc9b9cb73e88de91feb2a35530f95fb0/c8a4f/llm-attack.webp 1000w,\n/static/fc9b9cb73e88de91feb2a35530f95fb0/11fc3/llm-attack.webp 1024w","type":"image/webp","sizes":"(min-width: 1000px) 1000px, 100vw"}]},"width":1000,"height":457.99999999999994}}},"title":"Misusing Tools in Large Language Models With Visual Adversarial Examples","date":"Jun 2023 - Present","tags":["LLM","Security"],"path":"research/llm_security","excerpt":"Large Language Models (LLMs) are being enhanced with the ability to use tools and to process multiple modalities. These new capabilities bring new benefits and also new security risks. In this work, we show that an attacker can use visual adversarial examples to cause attacker-desired tool usage. For example, the attacker could cause a victim LLM to delete calendar events, leak private conversations and book hotels. Different from prior work, our attacks can affect the confidentiality and integrity of user resources connected to the LLM while being stealthy and generalizable to multiple input prompts. We construct these attacks using gradient-based adversarial training and characterize performance along multiple dimensions. We find that our adversarial images can manipulate the LLM to invoke tools following real-world syntax almost always (~98%) while maintaining high similarity to clean images (~0.9 SSIM). Furthermore, using human scoring and automated metrics, we find that the attacks do not noticeably affect the conversation (and its semantics) between the user and the LLM.","links":[{"name":"Paper(Arxiv)","url":"https://arxiv.org/abs/2310.03185"}],"commit":0,"type":"research"},"fileAbsolutePath":"/home/runner/work/Reapor-Yurnero.github.io/Reapor-Yurnero.github.io/content/research/llm_security/index.md","fields":{"slug":{"html":"var _excluded = [\"components\"];\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n/* @jsxRuntime classic */\n/* @jsx mdx */\n\nvar _frontmatter = {\n  \"title\": \"Misusing Tools in Large Language Models With Visual Adversarial Examples\",\n  \"tags\": [\"LLM\", \"Security\"],\n  \"date\": \"Jun 2023 - Present\",\n  \"authors\": [{\n    \"name\": \"advised by Earlence Fernado and Taylor Bert.\"\n  }],\n  \"path\": \"research/llm_security\",\n  \"excerpt\": \"Large Language Models (LLMs) are being enhanced with the ability to use tools and to process multiple modalities. These new capabilities bring new benefits and also new security risks. In this work, we show that an attacker can use visual adversarial examples to cause attacker-desired tool usage. For example, the attacker could cause a victim LLM to delete calendar events, leak private conversations and book hotels. Different from prior work, our attacks can affect the confidentiality and integrity of user resources connected to the LLM while being stealthy and generalizable to multiple input prompts. We construct these attacks using gradient-based adversarial training and characterize performance along multiple dimensions. We find that our adversarial images can manipulate the LLM to invoke tools following real-world syntax almost always (~98%) while maintaining high similarity to clean images (~0.9 SSIM). Furthermore, using human scoring and automated metrics, we find that the attacks do not noticeably affect the conversation (and its semantics) between the user and the LLM.\",\n  \"selected\": true,\n  \"cover\": \"./llm-attack.png\",\n  \"links\": [{\n    \"name\": \"Paper(Arxiv)\",\n    \"url\": \"https://arxiv.org/abs/2310.03185\"\n  }],\n  \"priority\": -20\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n    props = _objectWithoutProperties(_ref, _excluded);\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }));\n}\n;\nMDXContent.isMDXComponent = true;","htmlEncrypted":"","nonce":""}}}},"pageContext":{"fileAbsolutePath":"/home/runner/work/Reapor-Yurnero.github.io/Reapor-Yurnero.github.io/content/research/llm_security/index.md","postPath":"research/llm_security","translations":[{"hreflang":"en","path":"/research/llm_security"}]}},"staticQueryHashes":["1552981879","2158328490","3013679938"],"slicesMap":{}}